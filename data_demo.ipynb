{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c66fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import IPython\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MELD dataset, https://affective-meld.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc697bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0\n",
    "u = 3\n",
    "video_id = 'dia'+str(d)+'_utt'+str(u)\n",
    "\n",
    "split = 'train' # 'train', 'dev', 'test'\n",
    "data_dir = '/mnt/ff1f01b3-85e2-407c-8f5d-cdcee532daa5/emodet_cache/MELD.Raw/'\n",
    "\n",
    "anno_text = pd.read_csv(osp.join(data_dir, f'{split}_sent_emo.csv'))\n",
    "images_path = data_dir + split + '_splits/frames/' + video_id + '/'\n",
    "audio_path = data_dir + split + '_splits/audio/' + video_id + '.mp3'\n",
    "\n",
    "anno_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display text and labels\n",
    "f_d = anno_text[anno_text['Dialogue_ID'] == d]\n",
    "f_u = f_d[anno_text['Utterance_ID'] == u]\n",
    "text_gt = f_u['Utterance'].item()\n",
    "print(f_u['Sr No.'])\n",
    "print(anno_text.iloc[845])\n",
    "print('=======================')\n",
    "print('Text: ', text_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display video frames\n",
    "img_list= os.listdir(images_path)\n",
    "print(len(img_list))\n",
    "img = Image.open(images_path + '00000003.jpg')\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f251fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display audio\n",
    "audio_sample_rate = 22050\n",
    "_wav, sr = librosa.load(audio_path, sr=audio_sample_rate, mono=True)\n",
    "plt.figure(figsize=(20, 5))\n",
    "librosa.display.waveshow(_wav, sr=sr)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a575a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35505812",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading trained wav2vec provided by torch\n",
    "device = 'cuda:0'\n",
    "fixed_len = 200000\n",
    "\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)\n",
    "\n",
    "# load audio files \n",
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "waveform = waveform.to(device)\n",
    "waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)[0].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9304898",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    emission, _ = model(waveform)\n",
    "    print(emission.shape)\n",
    "plt.imshow(emission[0].cpu().T, interpolation=\"nearest\")\n",
    "plt.title(\"Classification result\")\n",
    "plt.xlabel(\"Frame (time-axis)\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.pre_audio import GreedyCTCDecoder\n",
    "\n",
    "decoder = GreedyCTCDecoder(labels=bundle.get_labels())\n",
    "transcript = decoder(emission[0])\n",
    "print(transcript)\n",
    "print('gt: ', text_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d3a40c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9368\n",
      "torch.Size([8, 3, 224, 224])\n",
      "dict_keys(['utt_token', 'emotion_idx', 'sentiment_idx', 'audio_wav'])\n",
      "torch.Size([283, 768])\n"
     ]
    }
   ],
   "source": [
    "from data.meld_data import MELD\n",
    "\n",
    "dataset = MELD(target='multimodal_finetune')\n",
    "print(dataset.__len__())\n",
    "frame, target = dataset.__getitem__(0)\n",
    "print(frame.shape)\n",
    "print(target.keys())\n",
    "print(target['audio_wav'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16196405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.clip import EmotionCLIP\n",
    "ckpt_path = '/home/minxiao/workspace/emo_det/EmotionCLIP/checkpoints_download/emotionclip_latest.pt'\n",
    "backbone_ckpt_path = '/home/minxiao/workspace/emo_det/EmotionCLIP/src/pretrained/vit_b_32-laion2b_e16-af8dbd0c.pth'\n",
    "\n",
    "model = EmotionCLIP(\n",
    "    temporal_fusion='transformer',#'mean',\n",
    "    video_len=8,\n",
    "    backbone_checkpoint=backbone_ckpt_path,\n",
    "    reset_logit_scale = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emodet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
